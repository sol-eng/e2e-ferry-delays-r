---
title: "Clean and Validate Data"
format:
  email:
    toc: true
    toc-location: left
    anchor-sections: true
    code-fold: true
    code-overflow: wrap
    code-summary: "Show Code"
    code-tools: true
    code-link: true
editor_options: 
  chunk_output_type: console
  canonical: true
html-table-processing: none
---

## Goals

The goals of this activity are to:

-   use a environment variable to toggle state for development and testing
-   perform data cleaning and validation to prepare data for our model
-   define thresholds for alerting in data validation and send condition-based email alerts based on those thresholds

Our conditions will send an email based on the the following:

1.  `condition <- "STOP"`: the size and shape of the data is not as expected; something is wrong with the input data so the processing of data is stopped and no production data is updated

2.  `condition <- "WARN"`: data validation complete but had warnings

3.  `condition <- "OK"`: no issues

## Task 1 - Define a "condition override" so we can manually trigger a condition and see the resulting email

üîÑ Task

Use an environment variable called `CONDITION_OVERRIDE` to be used during development and testing which will:

-   prevent data from being written to the database
-   force the the corresponding conditional email to be send based on the selected condition

The benefit of using an environment variable is that it can be set / modified in the "Vars" pane on Connect, meaning you can test different conditions without changing the deployed code.

Our conditions are:

1.  `condition <- "STOP"`: the size and shape of the data is not as expected; something is wrong with the input data so the processing of data is stopped and no production data is updated

2.  `condition <- "WARN"`: data validation complete but had warnings

3.  `condition <- "OK"`: no issues

### Set environment variable locally

To set this environment variable locally during development, add it to your `.Renviron` file. To do this, run the following command in your R console:

```{r}
#| eval: false

usethis::edit_r_environ("project")
```

``` {.bash filename=".Renviron"}
# file: either ~/.Renviron or .Renviron in the project root
# set condition to either STOP, WARN, or OK to preview the conditional responses.
CONDITION_OVERRIDE=STOP
```

**Don't forget to restart your session to load your revised environment.**

üí° Remember, the `.Renviron` file does not get deployed with this content, and you should not commit it to git. It is a local file that sets environment variables for your R session. You will be able to set this variable in the "Vars" pane on Connect when you deploy the report if you want to implement an override.

### Print a notification if an override is in place

This will appear in the rendered report if an override is place.

```{r}
#| label: identify test mode
#| output: asis

conditions <- c("STOP","WARN","OK")

# set CONDITION_OVERRIDE={STOP|WARN|OK} in `.Renviron` to preview the conditional responses.
if(Sys.getenv("CONDITION_OVERRIDE") %in% conditions) { 
glue::glue("<h2>‚ùó‚ùó Report generated in test mode. 
Data not written to database but an email for condition {Sys.getenv('CONDITION_OVERRIDE')} is sent  ‚ùó‚ùó</h2>\n") }

```

## Task 2 - Read in raw data

## Setup

```{r}
#| label: setup

library(DBI)
library(RPostgres)
library(pointblank)
library(tidyverse)
library(glue)
library(gt)
library(usethis)
library(ferryfairy)

```

üîÑ Task

Retrieve the raw data from the database for cleaning and validating.

### Connect to the database

```{r}
#| label: make database connection
#| eval: false

# Run this code as-is
con <- dbConnect(RPostgres::Postgres(), 
                 host = Sys.getenv("DATABASE_HOST"), 
                 port = "5432", 
                 dbname = Sys.getenv("DATABASE_NAME_R"), 
                 user = Sys.getenv("DATABASE_USER_R"), 
                 password = Sys.getenv("DATABASE_PASSWORD_R"))
```

### Read in data

Because we are in a Workshop setting, we will pull the data into local memory using `collect()` just to remove one layer of abstraction. This is not necessary, and in general, most of the data validation and cleaning steps that follow can still be done against the data in the database.

```{r}
#| label: read in raw data
#| eval: false

vesselinfo_raw <- dplyr::tbl(con, "vesselinfo_raw") |> collect()

vesselhistory_raw <- dplyr::tbl(con, "vesselhistory_raw") |> collect()

weather_terminal_history_raw <- dplyr::tbl(con, "weather_terminal_history_raw") |> collect()

```

```{r}
library(pins)

board <- board_connect()

vesselinfo_raw <- board |> pin_read("katie.masiello@posit.co/vesselinfo_raw")

vesselhistory_raw <- board |> pin_read("katie.masiello@posit.co/vesselhistory_raw")

weather_terminal_history_raw <- board |> pin_read("katie.masiello@posit.co/weather_terminal_history_raw")
```

## Task 3 - Validate raw production data schema

üîÑ Task

All of our downstream tasks depend on our raw data coming in with the expected columns and schema. We could impose errors downstream if we process errant data.

-   Use pointblank to validate the weather and vessel history raw data has the expected columns and schema.
-   If the data is not as expected, stop processing and set a data integrity alert.
-   If the data is as expected, proceed with cleaning, transforming, and preparing the data for the model.

### Define a column schema so we can check data is as expected

```{r}
#| label: define expected schemas

schema_vesselhistory <- col_schema(
                                  vessel_id = "integer",
                                  vessel = "character",
                                  departing = "character",
                                  arriving = "character",
                                  scheduled_depart = "character",
                                  actual_depart = "character",
                                  est_arrival = "character",
                                  date = "character"
                               )


schema_weather <- col_schema(
                            terminal = "character",
                            lat = "numeric",
                            long = "numeric",
                            time = "character", 
                            precipitation = "numeric",
                            weather_code = "integer", 
                            cloud_cover_low = "integer",
                            wind_speed_10m = "numeric",
                            wind_gusts_10m = "numeric"
                            )

```

### Validate the raw data column schemas

If either raw data schema fail validation, it will generate a stop notice and prevent downstream processing.

```{r}
#| label: data schema validations

# These are our data set integrity validations. All of these trigger a stop notice under fail conditions.
# Troubleshooting: if either of these fail, look at the x_list$col_types and $col_names to see the discrepancy

# validate integrity of vessel history data frame
vesselhistory_df_integrity_agent <- 
  create_agent(vesselhistory_raw, label = "Inital validation of the vessel history set to confirm overall schema and size. If there are issues with this validation, further processing stops and an alert is triggered.") |> 
  # verify column schema 
  col_schema_match(schema_vesselhistory, 
                   label = "Is the column schema as expected?") |> 
  #Check that expected columns exist. We make a table in the preconditions using a table transform that is made up of the column names of our inspections table. Then compare those values to the set of schema_inspection names.
  col_vals_in_set(columns = value, 
                  set = names(schema_vesselhistory), 
                  preconditions = ~. %>% tt_tbl_colnames, 
                  label = "Are the expected columns in the data set?") |> 
  # verify there are A LOT of rows of data to be sure import didn't mess up. 
  col_vals_gte(columns = n, 
               value = 10000L, # an arbitrary high-ish number
               preconditions = ~. %>% tally,
               label = "Are there more than 10k rows in the data?") |>
  interrogate()

vesselhistory_df_integrity_agent

# validate integrity of weather data frame
weather_df_integrity_agent <- 
  create_agent(weather_terminal_history_raw, label = "Inital validation of the station weather history data to confirm overall schema and size. If there are issues with this validation, further processing stops and an alert is triggered.") |> 
  # verify column schema 
  # troubleshooting, if this fails, look at the x_list$col_types and $col_names to see the discrepancy
  col_schema_match(schema_weather, 
                   label = "Is the column schema as expected?") |> 
  #Check that expected columns exist. We make a table in the preconditions using a table transform that is made up of the column names of our inspections table. Then compare those values to the set of schema_inspection names.
  col_vals_in_set(columns = value, 
                  set = names(schema_weather), 
                  preconditions = ~. %>% tt_tbl_colnames, 
                  label = "Are the expected columns in the data set?") |> 
  # verify there are A LOT of rows of data to be sure import didn't mess up. 
  col_vals_gte(columns = n, 
               value = 10000L, # an arbitrary high-ish number
               preconditions = ~. %>% tally,
               label = "Are there more than 10k rows in the data?") |>
  interrogate()
weather_df_integrity_agent
```

### Set `condition <- "STOP"` or `stop_alert` if there are any issues

Condition `"STOP"` will result in an alert email indicating there was a data integrity issue. `stop_alert` will prevent future code chunks from rendering, including preventing writes to the database

```{r}
#| label: set condition or stop alert

# condition<-"STOP" will be set if CONDITION_OVERRIDE is set to STOP, or if either data frame integrity validations failed. This specifies which email will be sent.
if(any(Sys.getenv("CONDITION_OVERRIDE") == "STOP",
       !all_passed(vesselhistory_df_integrity_agent),
       !all_passed(weather_df_integrity_agent))){
  condition <- "STOP"
}

# set a boolean that will stop additional data processing if there are any data integrity issues
stop_alert <- any((if(exists("condition")){condition == "STOP"}), Sys.getenv("CONDITION_OVERRIDE") == "STOP")

```

## Task 4 - Clean and transform data

üîÑ Task

As long as the raw data integrity validations passed, clean and transform the data to prepare it for additional validation and modeling. Cleaning will include:

-   convert dates into a usable format
-   convert terminal names and vessel names to lowercase
-   bring terminal names into agreement across data frames
-   calculate departure delay - create route-pairs for grouping related records
-   join the weather data with the sailing history, which will become our training data.

### Clean data

Data cleaning steps are extraneous to the learning objectives of the Workshop. To simply this document, cleaning steps have been wrapped in a function in the `ferryfairy` package. .

```{r}
#| label: Clean data sets

if (stop_alert != TRUE) {
  weather_terminal_history <- weather_terminal_history_raw |> clean_weather_terminal() |> 
    # extra cleaning step
    filter(!(is.na(precipitation) & is.na(weather_code) & is.na(cloud_cover_low) & is.na(wind_speed_10m) & is.na(wind_gusts_10m)))
  weather_terminal_history

  vesselinfo <- vesselinfo_raw |> clean_vesselinfo()
  vesselinfo

  vesselhistory <- vesselhistory_raw |> clean_vesselhistory()
  vesselhistory
}
```

### Join weather data with vessel history

This data will be what we use for the model. We'll join the weather data to the vessel history data based on the terminal and the date and time. The weather data is reported on the hour, so we will create a column in the vesselhistory rounding the scheduled departure time to the closest hour.

```{r}
#| label: Join weather data to vesselhistory

if (stop_alert != TRUE) {
  vesselhistory_w_weather <- vesselhistory |> 
    # round the scheduled departure to the closest hour to align with weather data
    mutate(closest_hour = round_date(scheduled_depart, "hour")) |> 
    # join the weather data to the vessel history data based on terminal and time
    left_join(weather_terminal_history, by = c("departing" = "terminal", "closest_hour" = "time")) 

  vesselhistory_w_weather
}
```

## Task 5 - Validate the model data

üîÑ Task

Use `pointblank` to create a validation agent and interrogate.

Sometimes it's okay to have records fail a validation. Perhaps a value is out of range or there was an error in data entry. A reasonable number of failing records may be expected, however, if a large amount of records are failing validation, this could be an indicator of larger data quality problems.

We will set "action levels" for each validation step in this agent. Exceeding the action level threshold will set a flag for that step. A validation step can have one or more ation levels for "notify," "warn," or "stop." We will then use the presence of this warning flag to conditionally send an email alert if a larger fraction of our data failed validation than we expected.

### Validate to remove errant data before modeling

```{r}
#| label: Validate data to be used for the model
if (stop_alert != TRUE) {
  main_agent <- create_agent(vesselhistory_w_weather) |> 
    # all records are distinct
    rows_distinct() |> 
    # sailing date is prior to today
    col_vals_lte(columns = date, today(),
                label = "Sailing date prior to today",
                actions = action_levels(warn_at = 0.01)) |> 
    # vessel_name is one known from vesseldata
    col_vals_in_set(columns =  vessel, 
                    set = vesselinfo$vessel_name, 
                    label = "Vessel name is known",
                    actions = action_levels(warn_at = 0.01)) |>
    # a departure and arrival must be specified
    col_vals_not_null(columns = c(departing, arriving),
                      actions = action_levels(warn_at = 0.05)) |> 
    # departing and arriving terminals are known
    col_vals_in_set(columns = c(departing, arriving), 
                    set = c(unique(weather_terminal_history$terminal),NA),
                    label = "Terminal name is known",
                    actions = action_levels(warn_at = 0.1)) |> 
    # calculated delay is not null
    col_vals_not_null(columns = delay) |> 
    # a departure more than 5 minutes early is likely bad data
    col_vals_gte(columns = delay, -5,
                label = "Departed no more than 5 minutes early",
                actions = action_levels(warn_at = 0.01)) |>
    # a departure more than 120 minutes late is likely bad data
    col_vals_lte(columns = delay, 120,
                label = "Departed no more than 120 minutes late",
                actions = action_levels(warn_at = 0.01)) |> 
    # weather code is valid
    col_vals_between(columns = weather_code, 0, 99, na_pass = TRUE,
                    actions = action_levels(warn_at = 0.01)) |> 
    # cloud_cover_low is valid
    col_vals_between(columns = cloud_cover_low, 0, 100, na_pass = TRUE,
                    actions = action_levels(warn_at = 0.01)) |> 
    interrogate()

  main_agent

  x_list <- get_agent_x_list(main_agent)
}
```

### Set `condition` to determine which email to send

Condition `"WARN"` will result in an alert email indicating that a concerning level of records failed validation, but downstream processing still proceeded. Condition `"OK"` will result in an email indicating that all looks good

```{r}
#| label: set condition WARN or OK

# condition<-WARN will be set if it is in override or if any warnings are set, otherwise condition<-OK

if (stop_alert != TRUE) {
  if(Sys.getenv("CONDITION_OVERRIDE") %in% c("WARN","OK")){
    condition <- Sys.getenv("CONDITION_OVERRIDE")
  } else if(any(x_list$warn, na.rm = TRUE)){
    condition <- "WARN"
  } else condition <- "OK"
}
```

## Task 6 - Remove failed records from data set

üîÑ Task

Pointblank has identified all of the rows of `vesselhistory_w_weather` that passed and failed validation. Now remove those that failed so the data that is passed downstream to our modeling step is squeaky clean.

```{r}
#| label: sunder data

if (stop_alert != TRUE) {
  modeldata_validated <- get_sundered_data(main_agent, type = "pass")
  modeldata_failed_validation <- get_sundered_data(main_agent, type = "fail")
}
```

## Task 7 - Write cleaned and validated data to database

üîÑ Task

Write cleaned data to database

```{r}
#| label: write validated data to database
#| eval: false

if (stop_alert != TRUE & !Sys.getenv("CONDITION_OVERRIDE") %in% conditions) {

  my_username <- "___" #Fill in your username, workshop participants!

  my_df_name <- paste0("modeldata_validated", my_username)

  DBI::dbWriteTable(conn = con, # the connection
                    name = my_df_name, # the name of the table you will create in the DB
                    value = modeldata_validated,
                    overwrite = TRUE)
}
```

```{r}
board |> pin_write(modeldata_validated)
```

## Task 8 - Send Conditional Email

üîÑ Task

-   Publish this report to Posit Connect
-   To preview the email for development or testing purposes, change the value of `CONDITION_OVERRIDE` in either the `.Renviron` file to "STOP", "WARN", or "OK" in a local Workbench session, or by setting the environment variable in the `Vars` pane on Posit Connect
-   üí° If testing locally, look in the folder `email-preview` to see the HTML email preview

Below is how we generate the emails.

We have three scenarios:

1.  `condition <- STOP`: the size and shape of the data is not as expected; something is wrong with the input data so the processing of data is stopped and no production data is updated
2.  `condition <- WARN`: data validation complete but had warnings
3.  `condition <- OK`: no issues with validation

We will use a feature of Quarto that allows yaml to be written at any point in the document. With this, we will define which conditional email should be sent as metadata. Then the subject and body of the email will be selected using Quarto's ability to include or exclude content based on a metadata value.

### Write condition to metadata

Metadata to be written will be: `use_condition_{condition}_email: true`

We have created a helper function within the `ferryfairy` package to write this YAML metadata for us. Feel free to browse the package for details.

#### write metadata

```{r}
#| label: write metadata
metadata_key <- glue::glue("use_condition_{condition}_email")

ferryfairy::write_metadata(metadata_key, TRUE)

```

### Make a summary table

We will include this in our logging and some of the emails.

```{r}
#| label: include a summary table
#| echo: false

if (stop_alert != TRUE) {
summary_table_basic <- modeldata_validated |> 
  filter(route_id %in% c(1:5)) |>
  select(route, vessel,date, delay, route_id) |>
  arrange(route_id) |> 
  group_by(route) |>
  mutate(delay = as.numeric(delay), 
         avg_delay = as.numeric(mean(delay)), 
         num_vessels = n_distinct(vessel),
         num_sailings = n()) |>
  ungroup() |> 
  select(-vessel, -date, -delay) |> 
  distinct() |> 
  gt(rowname_col = "route", groupname_col = "route_id") |>
  tab_header(
    title = "Departure Delays Summary",
    subtitle = glue::glue("{min(modeldata_validated$date)} 
                          to {max(modeldata_validated$date)}")) |>
  tab_source_note(source_note = md("Data sourced from [WSDOT Traveler Information API](https://wsdot.wa.gov/traffic/api/) and modified for use from its original source.")) |> 
  cols_label(
    avg_delay = md("Average delay (min)"),
    num_vessels = "Vessels on route", 
    num_sailings = "Sailings in period"
  ) |> 
  fmt_number(columns = avg_delay, decimals = 1) |>
  fmt_number(columns = num_sailings, use_seps = TRUE, decimals = 0) |> 
  opt_all_caps()  |> 
  text_transform(
    locations = cells_stub(),
    fn = function(x) {
      x <- gsub("-", " ‚Üí ", x)
      x <- gsub("_", " ", x)
      x
    }) |> 
  text_transform(
    locations = cells_row_groups(),
    fn = function(x) " ") |> 
  opt_vertical_padding(scale = 0.2) |> 
  opt_horizontal_padding(scale = 1.5)

 summary_table_full <- modeldata_validated |> 
  filter(route_id %in% c(1:5)) |>
  select(route, vessel,date, delay, route_id) |>
  arrange(route_id) |> 
  group_by(route) |>
  mutate(delay = as.numeric(delay), 
         avg_delay = as.numeric(mean(delay)), 
         num_vessels = n_distinct(vessel),
         num_sailings = n()) |>
  ungroup() |> 
  select(-vessel) |> 
  pivot_wider(names_from = date, values_from = delay, values_fn = list) |> 
  gt(rowname_col = "route", groupname_col = "route_id") |>
  tab_header(
    title = "Departure Delays Summary",
    subtitle = glue::glue("{min(modeldata_validated$date)} 
                          to {max(modeldata_validated$date)}")) |>
  tab_source_note(source_note = md("Data sourced from [WSDOT Traveler Information API](https://wsdot.wa.gov/traffic/api/) and modified for use from its original source.")) |> 
  cols_label(
    avg_delay = md("Average delay (min)"),
    num_vessels = "Vessels on route", 
    num_sailings = "Sailings in period"
  ) |> 
  cols_nanoplot(
    columns = starts_with("202"),
    plot_type = "boxplot",
    new_col_name = "nanoplots",
    new_col_label = "delay",
    plot_height = "3em",
    options = nanoplot_options(
      # show_data_points = FALSE
    )) |> 
  fmt_number(columns = avg_delay, decimals = 1) |>
  fmt_number(columns = num_sailings, use_seps = TRUE, decimals = 0) |> 
  opt_all_caps()  |> 
  text_transform(
    locations = cells_stub(),
    fn = function(x) {
      x <- gsub("-", " ‚Üí ", x)
      x <- gsub("_", " ", x)
      x
    }) |> 
  text_transform(
    locations = cells_row_groups(),
    fn = function(x) " ") |> 
  opt_vertical_padding(scale = 0.2) |> 
  opt_horizontal_padding(scale = 1.5)
}
```

### Define the email

```{r}
subject = list(
  STOP = list(
    icon = I("‚ö†Ô∏è"),
    text = I("Data integrity issue")
  ),
  WARN = list(
    icon = I("‚ÑπÔ∏è"),
    text = I("Data updated with warnings")
  ),
  OK = list(
    icon = I("‚úÖ"),
    text = I("Data updated")
  )
)
```

::: email
::: subject
`{r} subject[condition][[1]]$icon` Ferry Project: `{r} subject[condition][[1]]$text`
:::

::: {.content-visible when-meta="use_condition_STOP_email"}
## ‚ö†Ô∏è There was a data integrity issue with the project data on `{r} today()`

The data integrity validations check that the raw vessel history and terminal weather data have the expected column names and schema, and a minimum number of rows are present.

The validations for `{r} today()` **failed**, indicating a data integrity issue.

Further processing of the data did not take place. **Downstream content and artifacts have not been updated.**

Please review the incoming raw data to diagnose the issue.

The results of the data frame validation are shown below:

```{r}
#| echo: false

vesselhistory_df_integrity_agent

weather_df_integrity_agent
```
:::

::: {.content-visible when-meta="use_condition_WARN_email"}
## ‚ÑπÔ∏è Ferry data validation complete for `{r} today()` with warnings

The ferry data for modeling has been cleaned, validated, and written, but there were warnings set because the threshold for failure was exceeded.

### Validation failures

The table below summarizes the validation step(s) that triggered warnings.

```{r}
#| label: get failing validations details
#| echo: false

if (condition == "WARN") {
  step <- x_list$i[which(x_list$warn)]
  desc <- x_list$briefs[which(x_list$warn)]
  fails <- x_list$f_failed[which(x_list$warn)]
  threshold <- purrr::map(step, ~{x_list$validation_set$actions[[.x]]$warn_fraction}) |> unlist()

  warnings_table <- data.frame(step, desc,threshold,fails) |> gt() |> cols_label(step="Validation Step",desc="Description",fails="Fraction Failed", threshold="Threshold") |> fmt_number(columns = c(fails, threshold),decimal = 2)

  warnings_table |> as_raw_html() 
}
```

Review the associated extract files to see the records that failed validation.

```{r}
#| label: get extracts and attach to email
#| output: asis
#| echo: false

if (condition == "WARN") {
  failures <- purrr::map(step, ~{get_data_extracts(main_agent, .x)})

  purrr::walk2(failures, step, ~{write_csv(.x, file = glue("extracts_step_{.y}.csv"))})

  filenames_yaml <- glue(" - extracts_step_{step}.csv") |> str_c(collapse = "\n")

  ferryfairy::write_metadata("email-attachments", filenames_yaml)
}
```

### Summary of data written

# The table below summarizes the data written.

```{r}
#| echo: false


if (condition == "WARN") {
  summary_table_basic 
}


```
:::

::: {.content-visible when-meta="use_condition_OK_email"}
## ‚õ¥Ô∏è Ferry data validation complete for `{r} today()`

The ferry data for modeling has been cleaned, validated, andwritten to the database.

A summary of the data is shown below.

```{r}
#| label: summary of data
#| echo: false


if (condition == "OK") {
  summary_table_basic 
}


```
:::
:::

## Logging information

**Report run** `{r} today()`

**Condition set:** `{r} condition`

**Summary of results:**

```{r}
#| label: logging summary
#| echo: false
#| output: asis

if (condition == "STOP") {
  print(glue("Data integrity validation failed. Data processing for the model training did not proceed. An email notification of the failure was sent."))
}

if (condition == "WARN") {
  print(glue("Vessel history and weather data was validated and sundered, however a warning was triggered due to one or more validation steps exceeding the threshold for allowed failures.

Records removed that failed validation: {nrow(modeldata_failed_validation)}

Records written to database: {nrow(modeldata_validated)}

Warnings triggered for validation step(s) listed below:"))

  warnings_table
}

if (condition == "OK"){
  print(glue("Vessel history and weather data was validated and sundered.

Records removed that failed validation: {nrow(modeldata_failed_validation)}

Records written to database: {nrow(modeldata_validated)}"))
}



```

```{r}
#| echo: false

if (stop_alert != TRUE) {
  
  summary_table_full
}
```

## Preview of email sent:

<iframe src="email-preview/index.html" width="100%" height="500">

</iframe>
